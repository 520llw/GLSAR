"""
æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒæ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import torch
from collections import Counter, defaultdict
import numpy as np
from tqdm import tqdm

from config.sar_ship_config import cfg
from data.loaders import get_train_dataloader


def analyze_dataset_distribution():
    """è¯¦ç»†åˆ†ææ•°æ®é›†åˆ†å¸ƒ"""
    print("="*80)
    print("ğŸ“Š SARèˆ°èˆ¹æ•°æ®é›†åˆ†å¸ƒåˆ†æ")
    print("="*80)
    
    # 1. ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶
    print("\n1ï¸âƒ£ ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶...")
    label_files = list(cfg.TRAIN_LABEL_DIR.glob('*.txt'))
    
    class_counter = Counter()
    samples_per_class = defaultdict(list)
    empty_files = 0
    total_objects = 0
    
    for label_file in tqdm(label_files, desc="æ‰«ææ ‡ç­¾"):
        with open(label_file, 'r') as f:
            lines = f.readlines()
        
        if not lines:
            empty_files += 1
            continue
        
        file_classes = []
        for line in lines:
            parts = line.strip().split()
            if len(parts) >= 5:
                class_id = int(parts[0])
                class_counter[class_id] += 1
                file_classes.append(class_id)
                total_objects += 1
        
        for cls in set(file_classes):
            samples_per_class[cls].append(label_file.stem)
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“‹ æ ‡ç­¾æ–‡ä»¶ç»Ÿè®¡:")
    print(f"   æ€»æ ‡ç­¾æ–‡ä»¶: {len(label_files)}")
    print(f"   ç©ºæ ‡ç­¾æ–‡ä»¶: {empty_files}")
    print(f"   æœ‰æ•ˆæ–‡ä»¶: {len(label_files) - empty_files}")
    print(f"   æ€»ç›®æ ‡æ•°: {total_objects}")
    
    print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
    print(f"{'ç±»åˆ«ID':<8} {'ç±»åˆ«åç§°':<12} {'ç›®æ ‡æ•°':<10} {'æ ·æœ¬æ•°':<10} {'å æ¯”':<10}")
    print("-"*60)
    
    for class_id in range(cfg.NUM_CLASSES):
        class_name = cfg.CLASSES[class_id]
        count = class_counter.get(class_id, 0)
        num_samples = len(samples_per_class.get(class_id, []))
        percentage = (count / total_objects * 100) if total_objects > 0 else 0
        
        print(f"{class_id:<8} {class_name:<12} {count:<10} {num_samples:<10} {percentage:<10.2f}%")
    
    # 2. æ£€æŸ¥ä¸å¹³è¡¡ç¨‹åº¦
    print(f"\nâš ï¸  æ•°æ®ä¸å¹³è¡¡åˆ†æ:")
    counts = [class_counter.get(i, 0) for i in range(cfg.NUM_CLASSES)]
    if max(counts) > 0 and min(counts) >= 0:
        imbalance_ratio = max(counts) / max(min(counts), 1)
        print(f"   æœ€å¤§ç±»åˆ«/æœ€å°ç±»åˆ«æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
        
        if imbalance_ratio > 10:
            print(f"   ğŸš¨ ä¸¥é‡ä¸å¹³è¡¡ï¼(>{10}:1)")
        elif imbalance_ratio > 5:
            print(f"   âš ï¸  ä¸­åº¦ä¸å¹³è¡¡ (>{5}:1)")
        elif imbalance_ratio > 2:
            print(f"   âš ï¸  è½»åº¦ä¸å¹³è¡¡ (>{2}:1)")
        else:
            print(f"   âœ… åˆ†å¸ƒè¾ƒå‡è¡¡")
    
    # 3. æµ‹è¯•æ•°æ®åŠ è½½å™¨çš„æ‰¹æ¬¡åˆ†å¸ƒ
    print(f"\n2ï¸âƒ£ æµ‹è¯•æ•°æ®åŠ è½½å™¨æ‰¹æ¬¡åˆ†å¸ƒ...")
    
    loader = get_train_dataloader()
    
    batch_class_dist = []
    num_batches_to_check = min(100, len(loader))
    
    for i, batch in enumerate(tqdm(loader, total=num_batches_to_check, desc="æ£€æŸ¥batches")):
        if i >= num_batches_to_check:
            break
        
        all_labels = torch.cat(batch['gt_labels'])
        if len(all_labels) > 0:
            batch_classes = all_labels.unique().tolist()
            batch_class_dist.append(len(batch_classes))
    
    print(f"\nğŸ“¦ å‰{num_batches_to_check}ä¸ªBatchçš„ç±»åˆ«åˆ†å¸ƒ:")
    print(f"   å¹³å‡æ¯batchåŒ…å«ç±»åˆ«æ•°: {np.mean(batch_class_dist):.2f}")
    print(f"   æœ€å°ç±»åˆ«æ•°/batch: {min(batch_class_dist)}")
    print(f"   æœ€å¤§ç±»åˆ«æ•°/batch: {max(batch_class_dist)}")
    
    if np.mean(batch_class_dist) < 2:
        print(f"   ğŸš¨ è­¦å‘Šï¼šæ¯ä¸ªbatchå¹³å‡ç±»åˆ«æ•°å¤ªå°‘ï¼")
        print(f"      è¿™ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œå»ºè®®:")
        print(f"      1. å¢åŠ batch_size")
        print(f"      2. ä½¿ç”¨ç±»åˆ«å‡è¡¡é‡‡æ ·")
    
    # 4. æ¨èçš„ç±»åˆ«æƒé‡
    print(f"\n3ï¸âƒ£ æ¨èçš„ç±»åˆ«æƒé‡é…ç½®:")
    
    if total_objects > 0:
        # è®¡ç®—é€†é¢‘ç‡æƒé‡
        weights = {}
        avg_count = total_objects / cfg.NUM_CLASSES
        
        print(f"\nğŸ“Š åŸºäºé€†é¢‘ç‡çš„æƒé‡å»ºè®®:")
        for class_id in range(cfg.NUM_CLASSES):
            class_name = cfg.CLASSES[class_id]
            count = class_counter.get(class_id, 1)
            weight = avg_count / count
            weights[class_name] = weight
            
            print(f"   '{class_name}': {weight:.2f},")
        
        print(f"\nğŸ’¡ å»ºè®®åœ¨ config/sar_ship_config.py ä¸­æ›´æ–° CLASS_WEIGHTS:")
        print(f"   CLASS_WEIGHTS = {{")
        for class_name, weight in weights.items():
            print(f"       '{class_name}': {weight:.2f},")
        print(f"   }}")
    
    # 5. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡‡æ ·ç­–ç•¥
    print(f"\n4ï¸âƒ£ é‡‡æ ·ç­–ç•¥å»ºè®®:")
    
    if imbalance_ratio > 5:
        print(f"   ğŸ¯ å»ºè®®ä½¿ç”¨ç±»åˆ«å‡è¡¡é‡‡æ · (Balanced Sampling)")
        print(f"   å®ç°æ–¹å¼:")
        print(f"   1. WeightedRandomSampler")
        print(f"   2. è¿‡é‡‡æ ·å°‘æ•°ç±»")
        print(f"   3. æ¬ é‡‡æ ·å¤šæ•°ç±»")
    elif imbalance_ratio > 2:
        print(f"   ğŸ’¡ å¯ä»¥è€ƒè™‘:")
        print(f"   1. è°ƒæ•´ç±»åˆ«æƒé‡")
        print(f"   2. ä½¿ç”¨Focal Loss (å·²å¯ç”¨)")
    else:
        print(f"   âœ… å½“å‰åˆ†å¸ƒå¯ä»¥ç›´æ¥è®­ç»ƒ")
    
    print("\n" + "="*80)
    print("åˆ†æå®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    analyze_dataset_distribution()